{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RAGO Documentation","text":"<p>Welcome to the RAGO (Retrieval Augmented Generation Optimizer) documentation!</p>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":"<pre><code>    \ud83d\udcc1 docs/\n    \u251c\u2500\u2500 \ud83d\udcc1 code_architecture                 # Code Architecture\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 overview.md\n    \u251c\u2500\u2500 \ud83d\udcc1 installation                      # Installation guide\n    \u2502   \u251c\u2500\u2500 \ud83d\udcc4 elasticsearch.md\n    \u2502   \u2514\u2500\u2500 \ud83d\udcc4 ollama.md\n    \u2514\u2500\u2500 \ud83d\udcc1 usage_guide                       # Usage guide\n        \u251c\u2500\u2500 \ud83d\udcc1 dataset                       # Generate and load datasets\n        \u2502   \u251c\u2500\u2500 \ud83d\udcc4 data_loader.md\n        \u2502   \u2514\u2500\u2500 \ud83d\udcc4 generator.md\n        \u251c\u2500\u2500 \ud83d\udcc1 evaluation                    # Evaluation and metrics\n        \u2502   \u2514\u2500\u2500 \ud83d\udcc4 metrics.md\n        \u251c\u2500\u2500 \ud83d\udcc1 optimization                  # Optimization methods and strategies\n        \u2502   \u251c\u2500\u2500 \ud83d\udcc4 run_experiment.md\n        \u2502   \u2514\u2500\u2500 \ud83d\udcc4 tpe.md\n        \u2514\u2500\u2500 \ud83d\udcc1 rag                           # RAG concepts, configurations and components\n            \u251c\u2500\u2500 \ud83d\udcc4 rag_concepts.md\n            \u251c\u2500\u2500 \ud83d\udcc4 rag_configuration.md\n            \u251c\u2500\u2500 \ud83d\udcc4 reader.md\n            \u2514\u2500\u2500 \ud83d\udcc4 retriever.md\n</code></pre>"},{"location":"#quick-navigation","title":"\ud83c\udfaf Quick Navigation","text":"### \ud83d\ude80 Getting Started - **[Installation](installation/ollama.md)** - Setup &amp; ollama configuration - **[Quick Start](usage_guide/optimization/run_experiment.md)** - Your first optimization     ### \ud83d\udcd6 Core Documentation - **[RAG Concepts](usage_guide/rag/rag_concepts.md)** - Understanding RAG - **[RAG Configuration](usage_guide/rag/rag_configuration.md)** - Parameters &amp; search space - **[Retriever](usage_guide/rag/retriever.md)** - Retrieval methods - **[Reader](usage_guide/rag/reader.md)** - Generation strategies     ### \u2699\ufe0f Optimization - **[Run Optimization](usage_guide/optimization/run_experiment.md)** - Optimization parameters and strategies - **[TPE Algorithm](usage_guide/optimization/tree_parzen_estimator.md)** - Bayesian optimization theory     ### \ud83d\udd27 Advanced - **[Dataset Loader &amp; Format](usage_guide/dataset/data_loader.md)** - Dataset Loading and format - **[Dataset Generator](usage_guide/dataset/generator.md)** - Dataset Generators - **[Evaluation](usage_guide/evaluation/metrics.md)** - Metrics &amp; evaluators"},{"location":"#core-concepts","title":"\ud83d\udd2c Core Concepts","text":"<p>RAG (Retrieval Augmented Generation) combines: 1. Retrieve relevant documents from knowledge base 2. Augment LLM prompt with context 3. Generate informed answers</p> <p>RAG Optimization automatically finds the best configuration (retriever, embeddings, LLM params) for your use case using Bayesian Optimization.</p> <p>\u2192 Learn more: RAG Concepts | Config Space</p>"},{"location":"#external-resources","title":"\ud83d\udcd6 External Resources","text":""},{"location":"#research-papers","title":"Research Papers","text":"<ul> <li>Tree-structured Parzen Estimator - TPE optimization algorithm</li> <li>BERTScore - Semantic evaluation metrics</li> <li>LLM-as-a-Judge - Using LLMs for evaluation</li> </ul>"},{"location":"#related-projects","title":"Related Projects","text":"<ul> <li>Optuna - Hyperparameter optimization framework</li> <li>LangChain - LLM application framework</li> <li>LlamaIndex - Data framework for LLMs</li> <li>Ollama - Run LLMs locally</li> </ul>"},{"location":"#need-help","title":"\ud83d\udca1 Need Help?","text":"<ul> <li>\ud83d\udcac Ask in GitHub Discussions</li> <li>\ud83d\udc1b Report bugs in Issues</li> </ul>"},{"location":"code_architecture/overview/","title":"Code Architecture Overview","text":"<p>This document provides a high-level overview of RAGO's code architecture and design patterns.</p>"},{"location":"code_architecture/overview/#project-structure","title":"\ud83d\udce6 Project Structure","text":"<pre><code>rago/\n\u251c\u2500\u2500 data_objects/           # Core data structures (RAGOutput, EvalSample, Metric)\n\u251c\u2500\u2500 dataset/                # Dataset handling and generation\n\u2502   \u2514\u2500\u2500 generator/           # Automatic dataset generation from documents\n\u251c\u2500\u2500 eval/                   # Evaluation framework and metrics\n\u251c\u2500\u2500 model/                  # Model wrappers and configurations\n\u2502   \u251c\u2500\u2500 configs/             # Configuration dataclasses\n\u2502   \u2514\u2500\u2500 wrapper/             # Unified interfaces for different frameworks\n\u251c\u2500\u2500 optimization/           # Optimization engine\n\u2502   \u251c\u2500\u2500 manager/             # Optimization managers (Direct, Pairwise)\n\u2502   \u2514\u2500\u2500 search_space/        # Parameter search spaces\n\u251c\u2500\u2500 prompts/                # Prompt templates and configurations\n\u2514\u2500\u2500 utils/                  # Utility functions\n</code></pre>"},{"location":"code_architecture/overview/#design-principles","title":"\ud83d\udca1 Design Principles","text":""},{"location":"code_architecture/overview/#separation-of-concerns","title":"Separation of Concerns","text":"<ul> <li>Config: What you want</li> <li>Wrapper: How to build it</li> <li>Space: What to optimize</li> <li>Manager: How to optimize</li> </ul>"},{"location":"code_architecture/overview/#framework-agnostic","title":"Framework Agnostic","text":"<ul> <li>Core logic independent of LangChain/LlamaIndex</li> <li>Easy to add new frameworks via wrappers</li> </ul>"},{"location":"code_architecture/overview/#type-safety","title":"Type Safety","text":"<ul> <li>Dataclasses for configs</li> <li>Type hints everywhere</li> <li>Pydantic validation</li> </ul>"},{"location":"code_architecture/overview/#composability","title":"Composability","text":"<ul> <li>Small, focused components</li> <li>Easy to combine in different ways</li> <li>Clear interfaces</li> </ul>"},{"location":"code_architecture/overview/#extensibility","title":"Extensibility","text":"<ul> <li>Abstract base classes for extension points</li> <li>Plugin-like architecture for new components</li> </ul>"},{"location":"code_architecture/overview/#core-design-patterns","title":"\ud83c\udfaf Core Design Patterns","text":""},{"location":"code_architecture/overview/#config-pattern","title":"Config Pattern","text":"<p>All components use dataclass configs for configuration:</p> <pre><code>from dataclasses import dataclass\nfrom rago.model.configs import LangchainRetrieverConfig\n\n@dataclass\nclass LangchainRetrieverConfig:\n    \"\"\"Configuration for a retriever.\"\"\"\n    type: str                          # e.g., \"VectorIndexRetriever\"\n    similarity_function: str           # e.g., \"cosine\"\n    search_type: str                   # e.g., \"similarity_score_threshold\"\n    search_kwargs: dict                # e.g., {\"k\": 5, \"score_threshold\": 0.7}\n    encoder: HFEncoderConfig          # Embedding model config\n</code></pre> <p>Benefits: - \u2705 Type-safe configuration - \u2705 Validation at creation time - \u2705 Easy serialization/deserialization - \u2705 Clear documentation via type hints</p>"},{"location":"code_architecture/overview/#wrapper-pattern","title":"Wrapper Pattern","text":"<p>RAGO provides unified interfaces for different LLM/RAG frameworks:</p> <pre><code># Wrapper interface\nclass RAG(ABC):\n    \"\"\"Abstract base class for RAG systems.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str) -&gt; RAGOutput:\n        \"\"\"Query the RAG system.\"\"\"\n        pass\n\n# Concrete implementations\nclass LangchainRAG(RAG):\n    \"\"\"LangChain-based RAG implementation.\"\"\"\n    pass\n\nclass LlamaIndexRAG(RAG):\n    \"\"\"LlamaIndex-based RAG implementation.\"\"\"\n    pass\n</code></pre> <p>Benefits: - \u2705 Framework-agnostic optimization - \u2705 Easy to add new frameworks - \u2705 Consistent interface across implementations - \u2705 Swap implementations without changing optimization code</p>"},{"location":"code_architecture/overview/#space-pattern","title":"Space Pattern","text":"<p>Search spaces define optimization parameters:</p> <pre><code>from rago.optimization.search_space import RAGConfigSpace, RetrieverConfigSpace\nfrom rago.optimization.search_space.param_space import IntParamSpace, FloatParamSpace\n\n@dataclass\nclass RetrieverConfigSpace:\n    \"\"\"Defines the search space for retriever parameters.\"\"\"\n\n    top_k: IntParamSpace = IntParamSpace(low=1, high=10)\n    score_threshold: FloatParamSpace = FloatParamSpace(low=0.0, high=1.0)\n\n    def sample(self, trial: optuna.Trial) -&gt; RetrieverConfig:\n        \"\"\"Sample a configuration from this space.\"\"\"\n        return RetrieverConfig(\n            top_k=self.top_k.sample(trial),\n            score_threshold=self.score_threshold.sample(trial)\n        )\n</code></pre> <p>Benefits: - \u2705 Declarative parameter definition - \u2705 Type-safe parameter ranges - \u2705 Integration with Optuna - \u2705 Easy to extend with new parameters</p>"},{"location":"code_architecture/overview/#component-hierarchy","title":"\ud83c\udfd7\ufe0f Component Hierarchy","text":""},{"location":"code_architecture/overview/#model-layer","title":"Model Layer","text":"<pre><code>Config \u2192 Wrapper \u2192 Model\n\nLangchainRetrieverConfig \u2192 LangchainRetriever \u2192 Vector Store\nLangchainLLMConfig \u2192 LangchainLLM \u2192 Ollama/OpenAI\nRAGConfig \u2192 RAG (LangChain/LlamaIndex) \u2192 Complete RAG Pipeline\n</code></pre> <p>Flow: 1. Config: Defines what you want (dataclass) 2. Wrapper: Translates config to framework-specific code 3. Model: Actual LLM/retriever/RAG implementation</p> <p>Example: <pre><code># 1. Define config\nconfig = LangchainRetrieverConfig(\n    type=\"VectorIndexRetriever\",\n    search_kwargs={\"k\": 5}\n)\n\n# 2. Wrapper creates the actual retriever\nretriever = LangchainRetriever.from_config(config)\n\n# 3. Use the retriever\ndocs = retriever.retrieve(\"query\")\n</code></pre></p>"},{"location":"code_architecture/overview/#optimization-layer","title":"Optimization Layer","text":"<pre><code>SearchSpace \u2192 Manager \u2192 Optuna Study\n\nRAGConfigSpace \u2192 SimpleDirectOptunaManager \u2192 optuna.Study\n                  \u2193\n              Evaluator \u2192 Metric\n</code></pre> <p>Flow: 1. SearchSpace: Defines parameter ranges 2. Manager: Orchestrates optimization loop 3. Optuna: Suggests next configuration (TPE) 4. Evaluator: Scores RAG output 5. Feedback: Update Optuna model</p>"},{"location":"code_architecture/overview/#data-flow","title":"\ud83d\udd04 Data Flow","text":""},{"location":"code_architecture/overview/#complete-optimization-flow","title":"Complete Optimization Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     OPTIMIZATION LOOP                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  RAGConfigSpace  \u2502  Define parameter ranges\n    \u2502  (search space)  \u2502  (retriever, LLM, reader params)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Optuna Trial    \u2502  Suggest next configuration\n    \u2502  (TPE sampler)   \u2502  based on past results\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   RAGConfig      \u2502  Sampled configuration\n    \u2502  (specific vals) \u2502  {retriever: vector, k: 5, temp: 0.7...}\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   RAG Wrapper    \u2502  Build RAG system from config\n    \u2502  (instantiate)   \u2502  (retriever + LLM + reader)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   EvalSample     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502   RAG.query()    \u2502  Query RAG system\n    \u2502  (test question) \u2502         \u2502  (execute)       \u2502  with test sample\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                          \u2502\n                                          \u25bc\n                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2502   RAGOutput      \u2502  Generated answer\n                                 \u2502 (answer+context) \u2502  + retrieved contexts\n                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                          \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n    \u2502  Expected Answer \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 (ground truth)   \u2502         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   Evaluator      \u2502  Compare output vs expected\n                        \u2502 (BERTScore/LLM)  \u2502  (compute quality score)\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502     Metric       \u2502  Score: 0.85\n                        \u2502   (score: 0.85)  \u2502  (quality metric)\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502  Optuna Update   \u2502  Learn from result\n                        \u2502  (update model)  \u2502  Update probability model\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u2502 (repeat until convergence)\n                                 \u2502\n                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                                \u2502\n                                                                \u25bc\n                                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                    \u2502   Best Config    \u2502\n                                                    \u2502  (optimal RAG)   \u2502\n                                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"code_architecture/overview/#key-abstractions","title":"\ud83c\udfa8 Key Abstractions","text":""},{"location":"code_architecture/overview/#config-classes","title":"Config Classes","text":"<p>Location: <code>rago/model/configs/</code></p> <pre><code>@dataclass\nclass BaseConfig:\n    \"\"\"Base configuration class.\"\"\"\n    pass\n\n# Retriever configs\n@dataclass\nclass LangchainRetrieverConfig(BaseConfig):\n    type: str\n    search_kwargs: dict\n    encoder: HFEncoderConfig\n\n# LLM configs\n@dataclass\nclass LangchainLLMConfig(BaseConfig):\n    model: str\n    temperature: float\n    top_p: float\n\n# Complete RAG config\n@dataclass\nclass RAGConfig(BaseConfig):\n    retriever: LangchainRetrieverConfig\n    llm: LangchainLLMConfig\n    reader: ReaderConfig\n</code></pre> <p>Purpose: Type-safe, serializable, validatable configurations</p>"},{"location":"code_architecture/overview/#wrapper-classes","title":"Wrapper Classes","text":"<p>Location: <code>rago/model/wrapper/</code></p> <pre><code>class BaseWrapper(ABC):\n    \"\"\"Base wrapper for models.\"\"\"\n\n    @abstractmethod\n    def from_config(cls, config: BaseConfig):\n        \"\"\"Create instance from config.\"\"\"\n        pass\n\n# Example: RAG wrapper\nclass RAG(BaseWrapper):\n    def __init__(self, config: RAGConfig):\n        self.config = config\n        self.retriever = self._build_retriever()\n        self.llm = self._build_llm()\n\n    def query(self, query: str) -&gt; RAGOutput:\n        \"\"\"Execute RAG pipeline.\"\"\"\n        contexts = self.retriever.retrieve(query)\n        answer = self.llm.generate(query, contexts)\n        return RAGOutput(answer=answer, contexts=contexts, query=query)\n</code></pre> <p>Purpose: Abstract away framework-specific details</p>"},{"location":"code_architecture/overview/#space-classes","title":"Space Classes","text":"<p>Location: <code>rago/optimization/search_space/</code></p> <pre><code>@dataclass\nclass ConfigSpace(ABC):\n    \"\"\"Base class for configuration search spaces.\"\"\"\n\n    @abstractmethod\n    def sample(self, trial: optuna.Trial) -&gt; BaseConfig:\n        \"\"\"Sample a config from this space.\"\"\"\n        pass\n\n# Example: Retriever search space\n@dataclass\nclass RetrieverConfigSpace(ConfigSpace):\n    retriever_type: CategoricalParamSpace = Field(\n        default=CategoricalParamSpace(choices=[\"VectorIndexRetriever\"])\n    )\n    top_k: IntParamSpace = Field(default=IntParamSpace(low=1, high=10))\n    score_threshold: FloatParamSpace = Field(default=FloatParamSpace(low=0.0, high=1.0))\n\n    def sample(self, trial: optuna.Trial) -&gt; LangchainRetrieverConfig:\n        return LangchainRetrieverConfig(\n            type=self.retriever_type.sample(trial),\n            search_kwargs={\n                \"k\": self.top_k.sample(trial),\n                \"score_threshold\": self.score_threshold.sample(trial)\n            }\n        )\n</code></pre> <p>Purpose: Declarative optimization parameter definition</p>"},{"location":"code_architecture/overview/#manager-classes","title":"Manager Classes","text":"<p>Location: <code>rago/optimization/manager/</code></p> <pre><code>class BaseOptunaManager(ABC):\n    \"\"\"Base optimization manager.\"\"\"\n\n    def __init__(self, params: OptimParams, config_space: RAGConfigSpace):\n        self.params = params\n        self.config_space = config_space\n        self.study = self._create_study()\n\n    @abstractmethod\n    def objective(self, trial: optuna.Trial) -&gt; float:\n        \"\"\"Objective function for optimization.\"\"\"\n        pass\n\n    def optimize(self) -&gt; optuna.Study:\n        \"\"\"Run optimization.\"\"\"\n        self.study.optimize(self.objective, n_trials=self.params.n_iter)\n        return self.study\n\n# Direct optimization\nclass SimpleDirectOptunaManager(BaseOptunaManager):\n    def objective(self, trial: optuna.Trial) -&gt; float:\n        # Sample config from space\n        rag_config = self.config_space.sample(trial)\n\n        # Build RAG system\n        rag = RAG.from_config(rag_config)\n\n        # Evaluate on dataset\n        scores = []\n        for sample in self.dataset:\n            output = rag.query(sample.query)\n            metric = self.evaluator.evaluate(output, sample.expected_answer)\n            scores.append(metric.value)\n\n        return sum(scores) / len(scores)\n</code></pre> <p>Purpose: Orchestrate optimization loop</p>"},{"location":"code_architecture/overview/#extension-points","title":"\ud83d\udd0c Extension Points","text":""},{"location":"code_architecture/overview/#adding-a-new-retriever","title":"Adding a New Retriever","text":"<pre><code># 1. Define config\n@dataclass\nclass MyRetrieverConfig(BaseConfig):\n    param1: str\n    param2: int\n\n# 2. Create wrapper\nclass MyRetriever(BaseRetriever):\n    def __init__(self, config: MyRetrieverConfig):\n        self.config = config\n\n    def retrieve(self, query: str) -&gt; List[str]:\n        # Implementation\n        pass\n\n# 3. Add to search space\n@dataclass\nclass RetrieverConfigSpace:\n    retriever_type: CategoricalParamSpace = Field(\n        default=CategoricalParamSpace(\n            choices=[\"VectorIndexRetriever\", \"BM25Retriever\", \"MyRetriever\"]  # \u2190 Add here\n        )\n    )\n\n    def sample(self, trial: optuna.Trial):\n        retriever_type = self.retriever_type.sample(trial)\n\n        if retriever_type == \"MyRetriever\":\n            return MyRetrieverConfig(\n                param1=...,\n                param2=...\n            )\n        # ...\n</code></pre>"},{"location":"code_architecture/overview/#adding-a-new-evaluator","title":"Adding a New Evaluator","text":"<pre><code># 1. Inherit from BaseEvaluator\nfrom rago.eval import BaseEvaluator\n\nclass MyEvaluator(BaseEvaluator):\n    def evaluate(self, output: RAGOutput, expected: str) -&gt; Metric:\n        # Your evaluation logic\n        score = compute_score(output.answer, expected)\n\n        return Metric(\n            name=\"my_metric\",\n            value=score,\n            metadata={\"details\": \"...\"}\n        )\n\n# 2. Use in optimization\noptimizer = SimpleDirectOptunaManager.from_dataset(\n    evaluator=MyEvaluator(),  # \u2190 Your evaluator\n    metric_name=\"my_metric\",\n    # ...\n)\n</code></pre>"},{"location":"installation/elasticsearch/","title":"Elasticsearch","text":"<p>Coming soon...</p>"},{"location":"installation/ollama/","title":"Ollama","text":"<p>Coming soon...</p>"},{"location":"usage_guide/dataset/data_loader/","title":"Datasets","text":""},{"location":"usage_guide/dataset/data_loader/#load-datasets","title":"\ud83d\udcd4 Load Datasets","text":"<p>To get all the available dataset:</p> <pre><code>from rago.dataset import QADatasetLoader\n\nQADatasetLoader.list_available_datasets()\n</code></pre> <p>Then, to load a particular dataset simply call the load_dataset of <code>QADatasetLoader</code>.</p> <p><pre><code>from rago.dataset import QADataset\n\nds = QADataset.load_dataset(\"hotpot_qa\")\n</code></pre> The first argument is the class of the dataset you want to create <code>QAdataset</code> (i.e. only query and answer) or <code>RAGdataset</code> if you want the Retriever corpus as well:</p> <pre><code>from rago.dataset import RAGDataset\n\nrag_ds = RAGDataset.load_dataset(\"hotpot_qa\")\n</code></pre> <p>When the dataset is first loaded it is saved to <code>cache_dir</code> (default to \"./data\" at root dir) or <code>save_path</code> if specified. Then, when loading the dataset again with same caching arguments it will be simply loaded from the cached version.</p>"},{"location":"usage_guide/dataset/data_loader/#dataset-format","title":"\ud83d\udcd0 Dataset Format","text":"<p>A <code>QAdataset</code> has samples of type <code>list[EvalSample]</code>. This is because one element taken from a dataset is called a sample. So an element of the evaluation dataset is an eval sample. An <code>EvalSample</code> contains contains both the information necessary to obtain the RAG output and the information necessary to evaluate it:</p> <ul> <li>the <code>query</code> is what we want the RAG output to answer. It used both to generate the rag output and to evaluate the RAG output (e.g to evaluate the relevance of the RAG output).</li> <li>the <code>context</code> (Optional) used by the generator to generate the answer. It is used to evaluate the RAG output (e.g to evaluate the correctness of the answer).</li> <li>the <code>explanations</code> (Optional) eventually given by the generator, it gives further guidance to the judge .</li> <li>the <code>reference_answer</code> (Optional) the correct answer to the query or an answer to compare to.</li> <li>the <code>reference_score</code> (Optional) the score of the reference.</li> </ul> <p>Here an example usage of the EvalSample Object:</p> <pre><code>from rago.data_objects import EvalSample\n\neval_sample = EvalSample(\"How old is thomas\", context = [\"Thomas is 12.\", \"Thomas was born 12 years ago.\"])\n</code></pre> <p>To quickly access one argument of all samples from a dataset one can directly access it from the dataset:</p> <pre><code>ds[\"train\"].query\n</code></pre> <p>This returns a list containing the query of each element in the dataset.</p> <p>Finally sometimes we only want to use a subset of the dataset (for instance if the dataset is to big and apply the algorithm on the full dataset is prohibitively expansive). Then we can sample a smaller version of the dataset from the original dataset:</p> <pre><code>sampled_ds = ds[\"train\"].sample(size = 10, seed = 0)\n</code></pre> <p>For a <code>RAGDataset</code> we need to specify how the corpus should be constructed. It must contains all the reference documents otherwise some questions are unanswerable. But we can specify how many distractor documents (by opposition to ref documents) we want per sample with the param <code>max_num_distractor_documents_per_sample</code>.</p> <pre><code>sampled_rag_ds = rag_ds[\"train\"].sample(size = 10, seed = 0, max_num_distractor_documents_per_sample = 2)\n</code></pre>"},{"location":"usage_guide/dataset/data_loader/#custom-datasets-with-processors","title":"\ud83d\udd27 Custom Datasets with Processors","text":"<p>RAGO provides built-in loaders for popular datasets (HotpotQA and CRAG) with predefined formats. However, you can load any custom dataset by defining a processor that converts your data format into RAGO's internal structure. Processors are defined in Processor Codes and handle the transformation from raw dataset formats (JSON, CSV, HuggingFace datasets) to <code>EvalSample</code> objects. To add support for a new dataset, simply create a processor class that inherits from <code>BaseProcessor</code> and implements the required conversion logic.</p>"},{"location":"usage_guide/dataset/data_loader/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Dataset Generator</li> <li>Run your first experiment</li> </ul>"},{"location":"usage_guide/dataset/generator/","title":"Dataset Generator","text":""},{"location":"usage_guide/dataset/generator/#overview","title":"\ud83d\udce6 Overview","text":"<p>The Dataset Generator automatically creates question-answer pairs from your documents using an LLM. This is useful in two scenarios:</p> <ol> <li>No existing dataset: You only have documents raw Documents but no annotated question-answer pairs for evaluation.</li> <li>Dataset augmentation: You have an existing QA dataset and a corpus but want to generate additional samples from the same or new documents to increase coverage.</li> </ol> <p>Instead of manually creating evaluation samples, the generator leverages an LLM to: - Analyze your documents - Generate relevant questions - Extract accurate answers from the text - Create structured <code>EvalSample</code> objects ready for optimization</p> <p>Input: Raw documents or existing dataset Output: <code>RAGDataset</code> with generated question-answer pairs</p>"},{"location":"usage_guide/dataset/generator/#simple-generator","title":"\u2699\ufe0f Simple Generator","text":"<p>The simplest way to instantiate a <code>SimpleDatasetGenerator</code> is the following:</p> <pre><code>from rago.dataset.generator import SimpleDatasetGenerator\n\ndataset_generator = SimpleDatasetGenerator()\n</code></pre> <p>It will use an Ollama Langchain LLM agent with default parameters.</p> <p>Additionally, we can either:</p> <ul> <li>Pass an LLM config to the make method of <code>SimpleDatasetGenerator</code> to make a custom LLM specifically for the Dataset Generator:     <pre><code>from rago.dataset.generator import DatasetGeneratorConfig, SimpleDatasetGenerator\nfrom rago.model.configs.llm_config.langchain import LangchainOllamaConfig\n\nllm_config = LangchainOllamaConfig(model_name=\"smollm2:1.7b\", temperature= 0.0)\ndataset_generator_config = DatasetGeneratorConfig(llm_config)\n\ndataset_generator = SimpleDatasetGenerator.make(dataset_generator_config)\n</code></pre></li> <li>Or pass an existing LLM Agent to the <code>SimpleDatasetGenerator</code> (e.g to share a single LLM between multiple module):     <pre><code>from rago.dataset.generator import SimpleDatasetGenerator\nfrom rago.model.configs.llm_config.langchain import LangchainOllamaConfig\nfrom rago.model.wrapper.llm_agent.llm_agent_factory import LLMAgentFactory\n\nllm_config = LangchainOllamaConfig(model_name=\"phi3:3.8b-mini-128k-instruct-q8_0\")\nllm_agent = LLMAgentFactory.make(llm_config)\ndataset_generator = SimpleDatasetGenerator(llm_agent)\n</code></pre> <p>You can add and customize your own dataset generator that inherits from the class BaseDatasetGenerator</p> </li> </ul>"},{"location":"usage_guide/dataset/generator/#generate-from-dataset","title":"\ud83d\udcad Generate from Dataset","text":"<p>To generate a dataset from <code>seed_data</code>: <pre><code>from rago.data_objects import Document\n\ngenerated_dataset = dataset_generator.generate_dataset(sampled_rag_ds)\n</code></pre> To generate a dataset from a <code>seed_data</code> and directly save the generated_dataset:</p> <pre><code>dataset_generator.generate_and_save_dataset(\"./synth_rag_dataset.json\", sampled_rag_ds)\n</code></pre>"},{"location":"usage_guide/dataset/generator/#generate-from-documents","title":"\ud83d\udcc4 Generate from Documents","text":"<p><pre><code>from rago.dataset.generator import SimpleDatasetGenerator\nfrom rago.data_objects import Document\n\n# Raw documents\ndocuments = [\n    Document(text=\"Safety procedures for Model X-500: Always wear protective equipment...\"),\n    Document(text=\"Maintenance schedule: Weekly inspection of hydraulic systems...\"),\n    Document(text=\"Emergency shutdown: Press red button and follow evacuation protocol...\")\n]\n\n# Generator creation\ndataset_generator = SimpleDatasetGenerator(\n    number_questions_per_document=2\n)\n\n# Dataset Generation\ngenerated_dataset = dataset_generator.generate_dataset(seed_data=documents)\n\nprint(f\"Samples {generated_dataset.samples}\")\n</code></pre> The output of the generated dataset is a <code>RAGDataset</code> object and the samples are <code>EvalSample</code> type (see Eval Sample Class)</p>"},{"location":"usage_guide/dataset/generator/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Data Loader</li> <li>Run your first experiment</li> </ul>"},{"location":"usage_guide/evaluation/evaluator/","title":"Evaluators","text":"<p>Evaluators are the core components that measure the quality of RAG system outputs by comparing generated answers against expected results or ground truth. RAGO provides multiple evaluation strategies\u2014from simple metric-based evaluators (BERTScore, SimilarityScore) to LLM-as-judge evaluators\u2014allowing you to choose the most appropriate method for your use case and optimize toward specific quality criteria.</p>"},{"location":"usage_guide/evaluation/evaluator/#base-evaluators","title":"Base Evaluators","text":"<p>Evaluators are used to evaluate any <code>DataObject</code>. For evaluation they take as input one RAG output <code>evaluation</code> or two <code>pairwise_evaluation</code> and output a result as a either a single <code>Metric</code> or  a <code>dict[str, Metric]</code> object. A single Evaluator might return one or many metric.</p> <p>To get all the available evaluators simply run:</p> <pre><code>from rago.eval.base import BaseEvaluator\n\nBaseEvaluator.list_available_evaluators()\n# Returns: ['bert_score', 'similarity']\n</code></pre> <p>To load a specific evaluator simply call <code>load</code> from the BaseEvaluator with the evaluator name: <pre><code>from rago.eval.base import BaseEvaluator\n\nevaluator = BaseEvaluator.load('bert_score')\n</code></pre></p>"},{"location":"usage_guide/evaluation/evaluator/#available-evaluators","title":"Available Evaluators","text":""},{"location":"usage_guide/evaluation/evaluator/#bertscore","title":"BERTScore","text":"<p>The BERTScore evaluator uses BERT-based embeddings to compute the similarity between generated and reference answers. It returns three metrics: precision, recall, and F1 score.</p> <pre><code>from rago.eval import BertScore\nfrom rago.data_objects import EvalSample, RAGOutput\n\nevaluator = BertScore()\neval_sample = EvalSample(query=\"How old is thomas?\", reference_answer=\"Thomas is 12 years old.\")\noutput = RAGOutput(answer=\"Thomas is 12.\")\n\neval_result = evaluator.evaluate(output, eval_sample)\nprint(eval_result[\"precision\"].score)  # BERTScore precision\nprint(eval_result[\"recall\"].score)     # BERTScore recall\nprint(eval_result[\"f1\"].score)          # BERTScore F1\n</code></pre> <p>Key features: - Returns 3 metrics: <code>precision</code>, <code>recall</code>, <code>f1</code> - Based on BERT embeddings and cosine similarity - Independent evaluator (pairwise evaluation = 2 sequential evaluations) - Requires reference answer in the <code>EvalSample</code></p>"},{"location":"usage_guide/evaluation/evaluator/#similarityscore","title":"SimilarityScore","text":"<p>The SimilarityScore evaluator uses sentence transformers to compute the cosine similarity between generated and reference answers. It's faster than BERTScore and returns a single similarity score.</p> <pre><code>from rago.eval import SimilarityScore\nfrom rago.data_objects import EvalSample, RAGOutput\n\n# Default model: Qwen/Qwen3-Embedding-0.6B\nevaluator = SimilarityScore()\n\n# Or specify a custom model\nevaluator = SimilarityScore(model_name=\"BAAI/bge-large-en-v1.5\")\n\neval_sample = EvalSample(query=\"How old is thomas?\", reference_answer=\"Thomas is 12 years old.\")\noutput = RAGOutput(answer=\"Thomas is 12.\")\n\neval_result = evaluator.evaluate(output, eval_sample)\nprint(eval_result[\"similarity\"].score)  # Cosine similarity score\n</code></pre> <p>Key features: - Returns 1 metric: <code>similarity</code> (cosine similarity between embeddings) - Configurable sentence transformer model - Fast evaluation with normalized embeddings - Independent evaluator - Requires reference answer in the <code>EvalSample</code></p>"},{"location":"usage_guide/evaluation/evaluator/#relevancyevaluator","title":"RelevancyEvaluator","text":"<p>The relevancy evaluator evaluates only the retrieved context (obtained by the RAG based on the query) compared to the source context (used by the generator to generate query). More precisely it returns the proportion of the source context that is in the retrieved context. Therefore using it requires an <code>EvalSample</code> with <code>context</code>:</p> <p>The relevancy evaluator does not require any argument to be instantiated: <pre><code>from rago.data_objects import EvalSample\nfrom rago.eval import RelevancyEvaluator\n\neval_sample = EvalSample(\"How old is thomas\", context = [\"Thomas is 12.\", \"Thomas was born 12 years ago.\"])\n\nevaluator = RelevancyEvaluator()\n</code></pre> Below is an example of the relevancy evaluator usage: <pre><code>from rago.utils import Document, RAGOutput\n\neval_result = evaluator.evaluate(\n    RAGOutput(retrieved_documents=[Document(\"Thomas is 12.\")]),\n    eval_sample,\n)\neval_result[\"relevancy\"].score #Output: 0.5\n</code></pre> The Output in this case is 0.5 because the output has retrieved 1 out of the 2 source contexts (1/2 = 0.5).</p> <p>Key features: - Returns 1 metric: <code>relevancy</code> (proportion of source context retrieved) - Evaluates retrieval quality, not answer quality - Independent evaluator - Requires <code>context</code> in the <code>EvalSample</code></p>"},{"location":"usage_guide/evaluation/evaluator/#independent-vs-dependent-evaluators","title":"Independent vs Dependent Evaluators","text":""},{"location":"usage_guide/evaluation/evaluator/#independent-vs-dependent-evaluators_1","title":"Independent vs Dependent Evaluators","text":"<p>The relevancy evaluator is an <code>IndependentEvaluator</code>. This means evaluating two outputs with <code>pairwise_evaluation</code> is the same as evaluating both outputs independently: <pre><code>from rago.utils import Document, RAGOutput\n\neval_sample = EvalSample(\"How old is thomas\", context = [\"Thomas is 12.\", \"Thomas was born 12 years ago.\"])\nevaluator = RelevancyEvaluator()\n\noutput_1 = RAGOutput(retrieved_documents=[Document(\"Thomas is 12.\")])\noutput_2 = RAGOutput(retrieved_documents=[Document(\"Thomas is 13.\")])\n\neval_result_1 = evaluator.evaluate(output_1, eval_sample)\neval_result_2 = evaluator.evaluate(output_2, eval_sample)\n\npairwise_eval_result_1, pairwise_eval_result_2 = evaluator.evaluate_n_wise([output_1, output_2], eval_sample)\n\nassert eval_result_1[\"relevancy\"].score == pairwise_eval_result_1[\"relevancy\"].score\nassert eval_result_2[\"relevancy\"].score == pairwise_eval_result_2[\"relevancy\"].score\n</code></pre></p> <p>More generally, any Evaluator in RAGO is either an independent evaluator or a dependent evaluator:</p> <ul> <li>Independent evaluators inherit from <code>BaseIndependentEvaluator</code>. This implies that the pairwise evaluation is just two sequential direct evaluations. This is why the example above is true.</li> <li> <p>Examples: <code>BertScore</code>, <code>SimilarityScore</code>, <code>RelevancyEvaluator</code></p> </li> <li> <p>Dependent evaluators inherit from <code>BaseDependentEvaluator</code>. This means that the example above would not hold (pairwise evaluation produces different results than individual evaluations).</p> </li> <li>Examples: <code>SimpleLLMEvaluator</code>, <code>CoTLLMEvaluator</code></li> </ul>"},{"location":"usage_guide/evaluation/evaluator/#llm-as-judge-evaluators","title":"LLM-as-Judge Evaluators","text":"<p>LLM evaluators are evaluators that use a Large Language Model to generate scores. These are dependent evaluators because the LLM can provide different scores when comparing outputs pairwise versus evaluating them individually.</p>"},{"location":"usage_guide/evaluation/evaluator/#simplellmevaluator","title":"SimpleLLMEvaluator","text":"<p>[!CAUTION] In the following subsection we present the <code>SimpleLLMEvaluator</code> only. Several LLM Evaluator shall be implemented.</p> <p>The <code>SimpleLLMEvaluator</code> is an evaluator that uses an LLM to output a score between <code>min_score</code> and <code>max_score</code>. This evaluator only outputs a score without explanation.</p> <p>The evaluators can be instantiated with the same logics as <code>DatasetGenerator</code>:</p> <p>Default Instantiation: <pre><code>from rago.eval import SimpleLLMEvaluator\n\nevaluator = SimpleLLMEvaluator()\n</code></pre></p> <p>Instantiation with a LLMConfig: <pre><code>from rago.eval import SimpleLLMEvaluator\nfrom rago.eval.llm_evaluator.base import LLMEvaluatorConfig\nfrom rago.model.configs.llm_config.langchain import LangchainOllamaConfig\n\nllm_config = LangchainOllamaConfig(model_name=\"smollm2:1.7b\", temperature= 0.0)\nevaluator_config = LLMEvaluatorConfig(judge = llm_config)\nevaluator = SimpleLLMEvaluator.make(evaluator_config)\n</code></pre></p> <p>Instantiation with an existing LLM:</p> <pre><code>from rago.eval import SimpleLLMEvaluator\nfrom rago.model.configs.llm_config.langchain import LangchainOllamaConfig\nfrom rago.model.wrapper.llm_agent.llm_agent_factory import LLMAgentFactory\n\nllm_config = LangchainOllamaConfig(model_name=\"phi3:3.8b-mini-128k-instruct-q8_0\")\nllm_agent = LLMAgentFactory.make(llm_config)\nevaluator = SimpleLLMEvaluator(llm_agent)\n</code></pre> <p>Usage examples:</p> <ol> <li> <p>Individual evaluation:</p> <pre><code>from rago.data_objects import EvalSample, RAGOutput\n\neval_sample = EvalSample(query=\"How old is thomas?\")\neval_result = evaluator.evaluate(RAGOutput(\"Thomas is 12.\"), eval_sample)\neval_result[\"correctness\"].score\n</code></pre> </li> <li> <p>N-wise evaluation (pairwise or more):</p> <pre><code>from rago.data_objects import EvalSample, RAGOutput\n\neval_sample = EvalSample(query=\"How old is thomas\")\nrag_output_1 = RAGOutput(\"Thomas is 12.\")\nrag_output_2 = RAGOutput(\"Thomas is 13.\")\n\neval_results = evaluator.evaluate_n_wise([rag_output_1, rag_output_2], eval_sample)\nprint(eval_results[0][\"correctness\"].score, eval_results[1][\"correctness\"].score)\n</code></pre> </li> </ol> <p>Key features: - Returns 1 metric: <code>correctness</code> (LLM-judged score) - Dependent evaluator (pairwise evaluation can differ from individual) - Configurable LLM backend - Useful for nuanced quality assessment</p>"},{"location":"usage_guide/evaluation/evaluator/#summary-table","title":"Summary Table","text":"Evaluator Type Metrics Requires Reference Requires Context Speed <code>BertScore</code> Independent precision, recall, f1 \u2705 \u274c Fast <code>SimilarityScore</code> Independent similarity \u2705 \u274c Very Fast <code>RelevancyEvaluator</code> Independent relevancy \u274c \u2705 Very Fast <code>SimpleLLMEvaluator</code> Dependent correctness \u274c \u274c Slow (LLM)"},{"location":"usage_guide/evaluation/evaluator/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\ud83d\udcca Metrics</li> <li>\u2699\ufe0f RAG Configurations</li> </ul>"},{"location":"usage_guide/evaluation/metrics/","title":"Metric","text":"<p>As its name hints a <code>Metric</code> corresponds to the result of an evaluation. It contains a <code>score</code> and optionally an <code>explanation</code> of the score, see Metrics Dataclass</p> <p>Below is an example usage of the EvaluationResult:</p> <pre><code>from rago.data_objects import Metric\n\ncorrectness = Metric(5, explanation=\"The answer is perfect and targets all the key points of the query correctly\")\n</code></pre> <p>Metrics can be generated manually as in the example above or using an evaluator. An Evaluator outputs a dictionary of Metrics as it can output multiple score, see for instance the Sequential Evaluator</p>"},{"location":"usage_guide/evaluation/metrics/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\ud83d\udcd3 Evaluators</li> <li>\u2699\ufe0f RAG Configurations</li> </ul>"},{"location":"usage_guide/optimization/run_experiment/","title":"Run Optimization with Optuna","text":""},{"location":"usage_guide/optimization/run_experiment/#configure-your-optimization","title":"\ud83d\udee0\ufe0f Configure Your Optimization","text":"<p>RAGO uses Optuna as its optimization framework.</p> <p>Just a reminder on the configuration:</p> <p>[!CAUTION] - <code>TEST_OLLAMA_HOST</code> needs to be an env variable, \"https://ollama.myserver.i\" for instance - If you use the BM25 retriever you need to specify 2 env variables to use <code>OPEN SEARCH</code>. In our case we use elasticsearch, therefore here are our current variables:   - <code>OPENSEARCH_URL</code> is set to \"https://node.myserver.i/\"   - <code>OPENSEARCH_INDEX_NAME</code> is set to \"rago_xp\" for instance</p> <p>The main components of the optimization frameworks are:</p>"},{"location":"usage_guide/optimization/run_experiment/#the-basic-optimization-parameters","title":"The Basic optimization parameters","text":"<p><pre><code>from rago.optimization.manager import OptimParams\n\nparams = OptimParams(\n    experiment_name=\"my_experiment\",    # Name for saving results\n    n_startup_trials=50,                # Random trials before TPE starts\n    n_iter=1000,                          # Total number of trials\n)\n</code></pre> Parameters explained: - <code>experiment_name</code>: Unique identifier for your optimization run. Results are saved in <code>experiments/{experiment_name}/</code> - <code>n_startup_trials</code>: Number of random exploration trials before TPE learning begins. A rule is around 5-10% of the total number of trials   - Too few \u2192 TPE might focus on local optima   - Too many \u2192 Wastes time on random search   - Recommended: 10-20% of total trials - <code>n_iter</code>: Total number of configurations to test</p>"},{"location":"usage_guide/optimization/run_experiment/#the-optimization-manager","title":"The Optimization manager","text":"<p>The manager orchestrates the optimization process:</p> <pre><code>from rago.optimization.manager import SimpleDirectOptunaManager\n\noptimizer = SimpleDirectOptunaManager.from_dataset(\n    params=params,\n    dataset=ds,\n    evaluator=evaluator,\n    metric_name=\"bert_score_f1\",\n    config_space=config_space,\n    sampler=None,      # Optional: custom sampler (default: TPE)\n    pruner=None,       # Optional: custom pruner (default: MedianPruner)\n)\n</code></pre>"},{"location":"usage_guide/optimization/run_experiment/#default-sampler-and-pruner","title":"Default Sampler and Pruner","text":"<p>Sampler (default: <code>optuna.samplers.TPESampler</code>): - Controls how new configurations are suggested - TPE (Tree-structured Parzen Estimator) is the default - Balances exploration (random search) and exploitation (focusing on promising areas) - Uses <code>n_startup_trials</code> random trials, then switches to TPE</p> <p>Pruner: - Controls when to stop unpromising trials early - MedianPruner: Stops trials performing worse than the median of previous trials - Saves computation by abandoning bad configurations early - Especially useful for multi-step evaluations (e.g., evaluating on multiple test samples)</p>"},{"location":"usage_guide/optimization/run_experiment/#custom-samplerpruner-advanced","title":"Custom Sampler/Pruner (Advanced)","text":"<p>You can use several pruners and override sampler defaults for advanced control:</p> <pre><code>import optuna\n\n# Custom TPE with specific parameters\ncustom_sampler = optuna.samplers.TPESampler(\n    n_startup_trials=50,\n    multivariate=True,      # Model parameter interactions\n    seed=42,                # Reproducibility\n)\n\n# More aggressive pruning\ncustom_pruner = optuna.pruners.MedianPruner(\n    n_startup_trials=50,\n    n_warmup_steps=3,       # Don't prune until 3 evaluations\n)\n\noptimizer = SimpleDirectOptunaManager.from_dataset(\n    params=params,\n    dataset=ds,\n    evaluator=evaluator,\n    metric_name=\"bert_score_f1\",\n    config_space=config_space,\n    sampler=custom_sampler,\n    pruner=custom_pruner,\n)\n</code></pre>"},{"location":"usage_guide/optimization/run_experiment/#complete-example","title":"\ud83d\udcd4 Complete Example","text":""},{"location":"usage_guide/optimization/run_experiment/#optimization-from-a-rag-dataset","title":"Optimization from a RAG dataset","text":"<pre><code>from rago.dataset import QADatasetLoader, RAGDataset\nfrom rago.eval import BertScore\nfrom rago.optimization.manager import OptimParams, SimpleDirectOptunaManager\nfrom rago.optimization.search_space.rag_config_space import RAGConfigSpace\nfrom rago.optimization.search_space.retriever_config_space import RetrieverConfigSpace\n\n# 1. Configure optimization\nparams = OptimParams(\n    experiment_name=\"my_rag_optimization\",\n    n_startup_trials=50,     # 50 random trials\n    n_iter=1000,             # 1000 total trials\n)\n\n# 2. Load dataset and evaluator\nds = QADatasetLoader.load_dataset(RAGDataset, \"crag\").sample(10, 0, 50)\nevaluator = BertScore()\n\n# 3. Define search space\nconfig_space = RAGConfigSpace()\n\n# 4. Instantiate the optimizer (uses TPE sampler + no pruner by default)\noptimizer = SimpleDirectOptunaManager.from_dataset(\n    params=params,\n    dataset=ds,\n    evaluator=evaluator,\n    metric_name=\"bert_score_f1\",\n    config_space=config_space,\n)\n\n# 5. Start optimization\nstudy = optimizer.optimize()\n\n# 6. Get best configuration\nprint(f\"Best score: {study.best_value}\")\nprint(f\"Best config: {study.best_params}\")\n</code></pre>"},{"location":"usage_guide/optimization/run_experiment/#optimization-from-a-list-of-documents","title":"Optimization from a list of Documents","text":"<p>To instantiate the manager from a seed set of documents and generate synthetic documents:</p> <pre><code>from typing import cast\n\nfrom rago.dataset import QADatasetLoader, RAGDataset\nfrom rago.eval import SimpleLLMEvaluator\nfrom rago.optimization.manager import OptimParams, SimpleDirectOptunaManager\nfrom rago.optimization.search_space.llm_config_space import OllamaLLMConfigSpace\nfrom rago.optimization.search_space.param_space import CategoricalParamSpace\nfrom rago.optimization.search_space.rag_config_space import RAGConfigSpace\nfrom rago.optimization.search_space.reader_config_space import LangchainReaderConfigSpace\n\n# 1. Configure optimization\nparams = OptimParams(\n    n_iter = 10,\n)\n\n# 2. Get corpus data and evaluator\ncorpus = cast(RAGDataset, QADatasetLoader.load_dataset(RAGDataset, \"crag\")).corpus_docs[:200]\nevaluator = SimpleLLMEvaluator()\n\n# 3. Define search space\nconfig_space = RAGConfigSpace(\n    reader_space = LangchainReaderConfigSpace(\n        OllamaLLMConfigSpace(model_name=CategoricalParamSpace(choices=[\"smollm:1.7b\"])),\n    ),\n)\n\n# 4. Instantiate the optimizer (uses TPE sampler + no pruner by default)\noptimizer = SimpleDirectOptunaManager.from_seed_data(\n            params= params,\n            seed_data= corpus,\n            evaluator=evaluator,\n            metric_name=\"correctness\",\n            config_space = config_space,\n        )\n\n# 5. Start optimization\nstudy = optimizer.optimize()\n</code></pre> <p>You can also replace the <code>LlamaIndexReaderConfigSpace</code> by <code>LangchainReaderConfigSpace</code> to run the langchain reader. However, in addition to generation via the LLM, the <code>LlamaIndexReaderConfigSpace</code> provides more advanced capabilities, such as 'refine', 'compact', and 'summarize' methods, which process chunks and may involve multiple LLM calls.</p> <p>[!CAUTION] You can also replace the <code>SimpleDirectOptunaManager</code> by <code>SimplePairWiseOptunaManager</code> to run the pair-wise optimization.</p>"},{"location":"usage_guide/optimization/run_experiment/#to-go-further","title":"\u26f5 To go further","text":"<p>\ud83d\udc49 Pimp your RAG configuration space (retriever and reader)</p> <p>When you run an experiment with an already existing experiment name and with the same configuration spaces, Optuna will keep computing the experiment by using the previous iterations and computes.</p>"},{"location":"usage_guide/optimization/run_experiment/#optimization-logs","title":"\ud83d\udcc4 Optimization Logs","text":"<p>A log file exists in each experiment directory that is created and contains all the important optimization steps, for instance in <code>experiments/my_experiment/my_experiment.log</code>.</p>"},{"location":"usage_guide/optimization/run_experiment/#visualization","title":"\ud83d\udc40 Visualization","text":"<p>During the optimization process, in order to visualize the optimization details and identify hyperparameter importance, <code>Optuna</code> also offers an interactive dashboard to visualize the results. Here is how to use it (in directory experiments/my_experiment/ if exist):</p> <pre><code>optuna-dashboard sqlite:///experiments/my_experiment/study.db\n</code></pre>"},{"location":"usage_guide/optimization/run_experiment/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Optuna Documentation</li> <li>TPE Algorithm</li> <li>RAG Configurations</li> </ul>"},{"location":"usage_guide/optimization/tpe/","title":"Optimization with Tree-structured Parzen Estimator (TPE)","text":""},{"location":"usage_guide/optimization/tpe/#how-does-optimization-work","title":"\ud83c\udfaf How Does Optimization Work?","text":""},{"location":"usage_guide/optimization/tpe/#the-challenge","title":"The Challenge","text":"<p>A RAG system has hundreds of configurable parameters:</p> <p><pre><code># Just a few examples:\nretriever_type = [\"vector\", \"bm25\", \"hybrid\"]\nembedding_model = [\"bge-m3\", \"e5-large\", \"qwen-embedding\", ...]\ntop_k = [1, 2, 3, 4, 5, ...]\ntemperature = [0.0, 0.1, 0.2, ..., 2.0]\ntop_p = [0.1, 0.2, ..., 1.0]\n# ... and many more!\n</code></pre> Total possible combinations: Millions! \ud83d\ude31</p>"},{"location":"usage_guide/optimization/tpe/#a-solution-bayesian-optimization","title":"A Solution: Bayesian Optimization","text":"<p>Instead of trying random configurations, RAGO uses intelligent search:</p> <pre><code>1. Try a few random configurations\n2. Evaluate their performance\n3. Build a probabilistic model of \"what works\"\n4. Use the model to suggest the next configuration to try\n5. Repeat until convergence\n</code></pre>"},{"location":"usage_guide/optimization/tpe/#bayesian-optimization-framework","title":"\ud83e\udde0 Bayesian Optimization Framework","text":""},{"location":"usage_guide/optimization/tpe/#the-mathematical-problem","title":"The Mathematical Problem","text":"<p>Goal: Find configuration $x^*$ that maximizes objective function $f(x)$</p> <p>$$x^* = \\arg\\max_{x} f(x)$$</p> <p>Where: - $x$ = RAG configuration (retriever type, embeddings, LLM params, etc.) - $f(x)$ = performance score (BERTScore, LLM-as-Judge, etc.)</p> <p>Challenge: $f(x)$ is expensive to evaluate (requires running RAG + evaluation on full dataset)</p>"},{"location":"usage_guide/optimization/tpe/#the-bayesian-approach","title":"The Bayesian Approach","text":"<p>Instead of blindly testing configurations, Bayesian optimization:</p> <ol> <li>Builds a surrogate model $\\hat{f}(x)$ that approximates $f(x)$</li> <li>Uses the model to decide where to evaluate next (acquisition function)</li> <li>Updates the model with new results</li> <li>Repeats until convergence</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Bayesian Optimization Loop                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                     \u2502\n\u2502  1. Surrogate Model: P(score | config)              \u2502\n\u2502                                                     \u2502\n\u2502  2. Acquisition Function: Which config to try next? \u2502\n\u2502                                                     \u2502\n\u2502  3. Evaluate: Run RAG with suggested config         \u2502\n\u2502                                                     \u2502\n\u2502  4. Update Model: Incorporate new result            \u2502\n\u2502                                                     \u2502\n\u2502  5. Repeat until convergence                        \u2502\n\u2502                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Example iteration:</p> <pre><code>Iteration 1:  Try random config x\u2081 \u2192 score = 0.65 \u2192 update model\nIteration 2:  Model suggests x\u2082  \u2192 score = 0.72 \u2192 update model\nIteration 3:  Model suggests x\u2083  \u2192 score = 0.68 \u2192 update model\nIteration 4:  Model suggests x\u2084  \u2192 score = 0.81 \u2192 update model\n...\n</code></pre>"},{"location":"usage_guide/optimization/tpe/#acquisition-functions","title":"Acquisition Functions","text":"<p>Question: How do we decide where to evaluate next?</p> <p>Expected Improvement (EI): A common acquisition function</p> <p>$$\\text{EI}(x) = \\mathbb{E}[\\max(f(x) - f(x^+), 0)]$$</p> <p>Where $f(x^+)$ is the best value found so far.</p> <p>Interpretation: \"How much improvement do we expect at this point?\"</p> <p>Trade-off: - Exploitation: Try configs similar to current best (high expected score) - Exploration: Try different configs to avoid local optima (high uncertainty)</p>"},{"location":"usage_guide/optimization/tpe/#tree-structured-parzen-estimator-tpe","title":"\ud83c\udf32 Tree-structured Parzen Estimator (TPE)","text":"<p>RAGO uses TPE by default, a powerful Bayesian optimization algorithm designed for hyperparameter tuning.</p>"},{"location":"usage_guide/optimization/tpe/#how-tpe-works","title":"How TPE Works","text":"<p>Key Insight: Instead of modeling $P(\\text{score}|\\text{config})$ directly, TPE models two distributions:</p> <ol> <li>$l(x)$ = $P(\\text{config} | \\text{score} \\geq y^*)$ \u2192 \"good\" configurations</li> <li>$g(x)$ = $P(\\text{config} | \\text{score} &lt; y^*)$ \u2192 \"bad\" configurations</li> </ol> <p>Where $y^*$ is a threshold (e.g., top 25% of scores).</p>"},{"location":"usage_guide/optimization/tpe/#tpe-algorithm-steps","title":"TPE Algorithm Steps","text":"<pre><code>1. Separate trials into \"good\" and \"bad\" based on threshold y*\n\n2. Model parameter distributions:\n   l(x) = distribution of params in GOOD trials\n   g(x) = distribution of params in BAD trials\n\n3. Sample next configuration by maximizing:\n\n   x_next = argmax [ l(x) / g(x) ]\n\n   Meaning: \"Params that appear often in good trials\n            but rarely in bad trials\"\n\n4. Evaluate f(x_next) and add to trial history\n\n5. Repeat\n</code></pre>"},{"location":"usage_guide/optimization/tpe/#intuitive-example","title":"Intuitive Example","text":"<p>Imagine searching for treasure in a field:</p> Strategy Approach Random Search Dig holes randomly everywhere Grid Search Dig in a systematic grid pattern TPE Notice treasures found near water \u2192 focus there, but occasionally explore dry areas (in case of hidden patterns)"},{"location":"usage_guide/optimization/tpe/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>TPE maximizes the Expected Improvement using:</p> <p>$$\\text{EI}(x) \\propto \\frac{l(x)}{g(x)}$$</p> <p>Why this works: - High $l(x)$: Config $x$ appears often in successful trials - Low $g(x)$: Config $x$ appears rarely in failed trials - High ratio \u2192 high probability of improvement</p>"},{"location":"usage_guide/optimization/tpe/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>Good optimization balances two strategies:</p> <ul> <li>Exploitation: Focus on areas known to work well, i.e. high $l(x)$</li> <li>Exploration: Try new areas to avoid missing better solutions, i.e. high uncertainty in $g(x)$</li> </ul> <p>TPE naturally balances both through: 1. Startup trials ($n_{\\text{startup}}$): Pure random exploration 2. Probabilistic modeling: Uncertainty in $l(x)$ and $g(x)$ encourages exploration 3. Threshold $y^*$: Controls exploration-exploitation trade-off</p>"},{"location":"usage_guide/optimization/tpe/#tpe-vs-other-methods","title":"TPE vs. Other Methods","text":"Method Exploration Categorical Params Conditional Params Speed Random Search \u2705 Excellent \u2705 Yes \u2705 Yes \u26a1 Fast Grid Search \u274c Systematic only \u2705 Yes \u274c Difficult \ud83d\udc0c Very Slow Gaussian Process \u2705 Good \u274c Difficult \u274c Difficult \ud83d\udc0c Slow (high-dim) TPE \u2705 Good \u2705 Yes \u2705 Yes \u26a1 Fast <p>TPE vs Random Search: While Random Search explores uniformly across the entire parameter space, TPE learns from previous trials to focus exploration on promising regions. Random Search needs many more trials to find good configurations by chance, whereas TPE achieves better results with fewer trials by modeling which parameter combinations lead to success. For RAG optimization with expensive evaluations (each trial requires running inference on the full dataset), TPE's sample efficiency\u2014typically finding good configurations in 30-50 trials vs 100+ for Random Search\u2014translates to significant time savings.</p> <p>Why TPE for RAG optimization? - Handles categorical parameters (retriever type, embedding model) - Handles conditional parameters (BM25 params only when using BM25, LLM params $\\eta$ and $\\tau$ if mirostat not None) - Fast even with many parameters - Proven on hyperparameter tuning tasks</p>"},{"location":"usage_guide/optimization/tpe/#convergence","title":"Convergence","text":"<p>Optimization stops when: 1. Max iterations reached (<code>n_iter</code>) 2. No improvement for N consecutive trials 3. Target metric achieved (optional early stopping)</p> <pre><code>Trials:  1   2   3   4   5   6   7   8   9   10\nScore:  .30 .45 .52 .68 .79 .80 .85 .86 .87 .87\n        \u2191   \u2191   \u2191   \u2191   \u2191   \u2191   \u2191   \u2191   \u2191   -\n\n        Large improvements \u2192 Small improvements \u2192 Convergence\n</code></pre>"},{"location":"usage_guide/optimization/tpe/#further-reading","title":"\ud83d\udcda Further Reading","text":""},{"location":"usage_guide/optimization/tpe/#research-papers","title":"Research Papers","text":"<ul> <li>TPE Algorithm: Bergstra et al. 2011 - Algorithms for Hyper-Parameter Optimization</li> <li>TPE Deep Dive: Watanabe 2023 - Tree-Structured Parzen Estimator: Understanding Its Algorithm Components</li> <li>Bayesian Optimization: Shahriari et al. 2016 - Taking the Human Out of the Loop: A Review of Bayesian Optimization</li> </ul>"},{"location":"usage_guide/optimization/tpe/#related-documentation","title":"Related Documentation","text":"<ul> <li>Run your optimization</li> <li>RAG Configurations</li> </ul>"},{"location":"usage_guide/rag/rag_concepts/","title":"RAG - Retrieval Augmented Generation Concepts","text":""},{"location":"usage_guide/rag/rag_concepts/#what-is-rag-retrieval-augmented-generation","title":"\ud83e\udd16 What is RAG (Retrieval Augmented Generation)?","text":""},{"location":"usage_guide/rag/rag_concepts/#the-basic-idea","title":"The Basic Idea","text":"<p>RAG enhances language models by giving them access to external knowledge.</p>"},{"location":"usage_guide/rag/rag_concepts/#without-rag","title":"Without RAG:","text":"<pre><code>User: \"What are the safety procedures for Model X-500?\"\nLLM: \"I don't have specific information about Model X-500...\"\n</code></pre>"},{"location":"usage_guide/rag/rag_concepts/#with-rag","title":"With RAG:","text":"<pre><code>User: \"What are the safety procedures for Model X-500?\"\n\n[System retrieves relevant documentation]\n\nLLM: \"According to the safety manual, Model X-500 requires:\n1. Lock-out tag-out procedures...\n2. Personal protective equipment including...\n3. ...\"\n</code></pre>"},{"location":"usage_guide/rag/rag_concepts/#rag-pipeline","title":"\ud83c\udfef RAG Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Query       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   RETRIEVAL      \u2502 \u2190 Search through docs\n\u2502  - Vector Search \u2502\n\u2502  - BM25, ...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Retrieved Docs  \u2502 \u2190 Docs for context\n\u2502  [Doc1, Doc2...] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   GENERATION     \u2502 \u2190 LLM uses docs to answer\n\u2502   (LLM + Docs)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Answer      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"usage_guide/rag/rag_concepts/#why-rag","title":"Why RAG?","text":"<ol> <li>Up-to-date Information: Access current documents without retraining</li> <li>Domain Specificity: Use your own proprietary knowledge</li> <li>Transparency: See which documents informed the answer</li> <li>Reduced Hallucinations: Grounded in actual documents</li> </ol>"},{"location":"usage_guide/rag/rag_concepts/#rag-vs-fine-tuning","title":"\u2705 RAG vs. Fine-tuning","text":"Aspect RAG Fine-tuning Data Changes Update documents immediately Requires retraining Cost Low (storage and some GPU) High (much GPUs, time) Transparency Can cite sources Black box Use Case Dynamic knowledge Specialized behavior"},{"location":"usage_guide/rag/rag_concepts/#rag-configurations","title":"\u2699\ufe0f RAG Configurations","text":"<p>The RAG configuration consists of: - A Splitter / Node Parser: during pre-processing phase, the splitter splits the document into chunks (Not implemented at this stage). - A Retriever: At inference, the retriever select chunks, create a context to answer a query. - A Reader / Synthesizer: Still in inference, the reader use the selected chunks to answer the query.</p> <p>\u26a0\ufe0f An infinite number of methods exist, only methods implemented at this stage are described in the following subsections.</p> <p>Here you have more information about RAG Configuration Space in RAGO \ud83d\udc49 RAG Configuration</p>"},{"location":"usage_guide/rag/rag_concepts/#splitter-not-implemented-yet","title":"Splitter (not implemented yet)","text":"<p>\u26a0\ufe0f <code>Sentence Splitter</code>, <code>Recursive Character Text Splitter</code>, <code>Semantic Splitter</code> shall be implemented in next versions.</p>"},{"location":"usage_guide/rag/rag_concepts/#retriever","title":"Retriever","text":"<p>Once we have a set of chunks we can then use a retriever to select chunks useful to answer a given query. More information on RAGO usage for the Retriever Configuration Spaces here \ud83d\udc49 Retriever Methods</p>"},{"location":"usage_guide/rag/rag_concepts/#reader","title":"Reader","text":"<p>Classical reader method uses the retrieved chunks, compact it in a single context block and use the LLM to generate the query directly. More complex reader methods proposed by <code>llamaIndex</code> library such as <code>Refine</code>, <code>Compact &amp; Refine</code>, <code>Tree Summarize</code> are implemented in the RAGO library. More information here \ud83d\udc49 Reader Methods</p>"},{"location":"usage_guide/rag/rag_concepts/#rag-output-in-rago","title":"\ud83d\udcc4 RAG Output in RAGO","text":"<p>A <code>RAGOutput</code> as its name hints is the output of RAG to a query. It contains:</p> <ul> <li>The <code>answer</code> of the RAG to the query.</li> <li>The <code>retrieved_documents</code> by the RAG to answer the query. Below his an example usage of the <code>RAGoutput</code> class:</li> </ul> <p><pre><code>from rago.data_objects import RAGOutput, RetrievedContext\n\nRAGOutput(\"Thomas is 12.\", [RetrievedContext(\"Thomas is 12.\")])\n</code></pre> RAG outputs can be generated manually or using a RAG.</p>"},{"location":"usage_guide/rag/rag_concepts/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\u2699\ufe0f RAG Configurations</li> <li>\ud83d\udd0d Retriever Methods</li> <li>\ud83e\udd16 Reader Methods</li> </ul>"},{"location":"usage_guide/rag/rag_configuration/","title":"RAG Configuration","text":""},{"location":"usage_guide/rag/rag_configuration/#rag-key-components","title":"\ud83e\udde9 RAG Key Components","text":"<p>The RAG configuration consists of: - A Splitter/ Node Parser: during pre-processing phase, the splitter splits the document into chunks (Not implemented at this stage). - A Retriever: At inference, the retriever select chunks, create a context to answer a query. - A Reader/ Synthesizer: Still in inference, the reader use the selected chunks to answer the query.</p> <p>\u26a0\ufe0f An infinite number of methods exist, only methods implemented at this stage are described in the following subsections.</p>"},{"location":"usage_guide/rag/rag_configuration/#splitter-not-implemented-yet","title":"Splitter (not implemented yet)","text":"<p>\u26a0\ufe0f <code>Sentence Splitter</code>, <code>Recursive Character Text Splitter</code>, <code>Semantic Splitter</code> shall be implemented in next versions.</p>"},{"location":"usage_guide/rag/rag_configuration/#retriever","title":"Retriever","text":"<p>Once we have a set of chunks we can then use a retriever to select chunks useful to answer a given query. Most retriever methods use embedding models in order to compute similarity between the query and the chunks. More information on RAGO usage for the Retriever Configuration Spaces here \ud83d\udc49 Retriever Methods</p>"},{"location":"usage_guide/rag/rag_configuration/#reader","title":"Reader","text":"<p>Classical reader method uses the retrieved chunks, compact it in a single context block and use the <code>LLM</code> to generate the query directly. More complex reader methods proposed by <code>llamaIndex</code> library such as <code>Refine</code>, <code>Compact &amp; Refine</code>, <code>Tree Summarize</code> are implemented in the RAGO library. More information about LLM parameters and reader strategies here \ud83d\udc49 Reader Methods.</p>"},{"location":"usage_guide/rag/rag_configuration/#rago-configuration-space","title":"\ud83d\udd2d RAGO Configuration Space","text":""},{"location":"usage_guide/rag/rag_configuration/#default-rag-configuration-space","title":"Default RAG Configuration Space","text":"<p><pre><code>from rago.optimization.search_space.rag_config_space import RAGConfigSpace\n\nconfig_space = RAGConfigSpace()\n</code></pre> When the <code>RAGConfigSpace</code> is instantiated without any argument, the retriever and reader default configuration spaces are respectively <code>RetrieverConfigSpace</code> and <code>LangchainReaderConfigSpace</code>:</p> <p><pre><code>RetrieverConfigSpace(\n    retriever_type_name=CategoricalParamSpace(name='RetrieverConfigSpace_retriever_type_name', choices=['VectorIndexRetriever']),\n    similarity_function=CategoricalParamSpace(name='RetrieverConfigSpace_similarity_function', choices=['cosine']),\n    search_type=CategoricalParamSpace(name='RetrieverConfigSpace_search_type', choices=['similarity_score_threshold']),\n    top_k=IntParamSpace(name='RetrieverConfigSpace_top_k', low=1, high=5, step=1, log=False),\n    score_threshold=FloatParamSpace(name='RetrieverConfigSpace_score_threshold', low=0.0, high=0.9, step=None, log=False),\n    encoder=HFEncoderConfigSpace(model_name=CategoricalParamSpace(name='encoder_model_name', choices=['BAAI/bge-m3', 'intfloat/e5-large-v2', 'Qwen/Qwen3-Embedding-0.6B', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2']))\n)\n</code></pre> <pre><code>LangchainReaderConfigSpace(\n    llm_config=OllamaLLMConfigSpace(\n        model_name=CategoricalParamSpace(name='OllamaLLMConfigSpace_model_name', choices=['smollm2:1.7b', 'qwen3:4b', 'gemma3:4b', 'llama3.2:3b']),\n        temperature=FloatParamSpace(name='OllamaLLMConfigSpace_temperature', low=0.0, high=1.0, step=None, log=False),\n        top_k=IntParamSpace(name='OllamaLLMConfigSpace_top_k', low=0, high=10000, step=1, log=False),\n        top_p=FloatParamSpace(name='OllamaLLMConfigSpace_top_p', low=0.0, high=1.0, step=None, log=False),\n        max_new_tokens=IntParamSpace(name='OllamaLLMConfigSpace_max_new_tokens', low=64, high=1024, step=1, log=False),\n        mirostat=CategoricalParamSpace(name='OllamaLLMConfigSpace_mirostat', choices=[0, 1, 2]),\n        mirostat_eta=FloatParamSpace(name='OllamaLLMConfigSpace_mirostat_eta', low=0.0, high=1.0, step=None, log=False),\n        mirostat_tau=FloatParamSpace(name='OllamaLLMConfigSpace_mirostat_tau', low=0.0, high=1.0, step=None, log=False),\n        num_ctx=IntParamSpace(name='OllamaLLMConfigSpace_num_ctx', low=64, high=12800, step=64, log=False),\n        repeat_last_n=IntParamSpace(name='OllamaLLMConfigSpace_repeat_last_n', low=-1, high=256, step=1, log=False),\n        base_url='TEST_OLLAMA_HOST', client_kwargs={'verify': False})\n)\n</code></pre></p>"},{"location":"usage_guide/rag/rag_configuration/#pimp-your-rag-configuration-space","title":"Pimp your RAG Configuration Space","text":""},{"location":"usage_guide/rag/rag_configuration/#retriever-methods","title":"Retriever methods","text":"<pre><code>from rago.optimization.search_space.param_space import CategoricalParamSpace\nfrom rago.optimization.search_space.reader_config_space import ReaderConfigSpace\nfrom rago.optimization.search_space.retriever_config_space import RetrieverConfigSpace\n\n# By default - Vector Index retriever only\nconfig_space = RAGConfigSpace(retriever_space=RetrieverConfigSpace())\n\n# BM25Retriever only\nconfig_space = RAGConfigSpace(RetrieverConfigSpace(\n    retriever_type_name=CategoricalParamSpace(choices=[\"BM25Retriever\"])\n    )\n)\n# Vector Index and BM25 retrievers in configuration space\nconfig_space = RAGConfigSpace(RetrieverConfigSpace(\n    retriever_type_name=CategoricalParamSpace(choices=[\"VectorIndexRetriever\", \"BM25Retriever\"])\n    )\n)\n</code></pre>"},{"location":"usage_guide/rag/rag_configuration/#reader-methods","title":"Reader methods","text":"<p>You can specify and choose the reader method from <code>llama Index</code> or <code>Langchain</code> as described here \ud83d\udc49 Reader Methods</p> <p>Regarding the retriever and reader methods it is also possible to modify the default values of the configuration spaces by specifying the class arguments directly.</p>"},{"location":"usage_guide/rag/rag_configuration/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\u2699\ufe0f RAG Concepts</li> <li>\ud83d\udd0d Retriever Methods</li> <li>\ud83e\udd16 Reader Methods</li> </ul>"},{"location":"usage_guide/rag/reader/","title":"Reader Methods and strategies","text":""},{"location":"usage_guide/rag/reader/#language-model-parameters","title":"\ud83c\udfb2 Language Model Parameters","text":""},{"location":"usage_guide/rag/reader/#definition","title":"Definition","text":"<p>The LLM computes the probability distribution over the vocabulary for the next token, given the previously generated tokens. During inference, we sample the next token $x_t$ according to this distribution.</p> <p>For a sequence $x_1, \\ldots, x_{t-1}$ and a candidate token $w_i$ from vocabulary $V$:</p> <p>$$P(x_t = w_i \\mid x_1, \\ldots, x_{t-1}) = \\frac{\\exp(z_i / T)}{\\sum_{j=1}^{|V|} \\exp(z_j / T)}$$</p> <p>where: - $x_1, \\ldots, x_{t-1}$: Previously generated tokens (context) - $x_t$: Next token to generate - $w_i$: The $i$-th token in vocabulary $V$ (candidate) - $z_i = f_\\theta(x_1, \\ldots, x_{t-1})_i$: Logit (raw score) for token $w_i$ from the model - $T$: Temperature parameter controlling randomness - $|V|$: Vocabulary size</p> <p>The parameter of the models are described in the following subsections.</p>"},{"location":"usage_guide/rag/reader/#temperature","title":"Temperature","text":"<ul> <li> <p>The idea of the temperature parameter $T$ is to modify the next token distribution probability describe in (1).</p> </li> <li> <p>As $T$ increases $\u2200 x \u2208 R^{d_{model}}$, $\\frac{x}{T}$ converge to $0$. This means the distribution converge to the uniform distribution as $T$ goes to infinity.</p> </li> <li> <p>Therefore, higher is the Temperature parameter the more noise it adds; When $T$ goes to $0$ the distribution becomes deterministic and gives all the density to the word with the highest probability. In this case we will always choose,</p> </li> </ul> <p>$$j^* = argmax_{j=1}^{|V|} z_j$$</p>"},{"location":"usage_guide/rag/reader/#top-k-sampling","title":"Top-k Sampling","text":"<p>$top_k$ allow to sample only from the $k$ most likely next tokens.</p> <p>\u26a0\ufe0f $k$ is fixed - When model is confident: $k$ might be too large - When model is uncertain: $k$ might be too small</p>"},{"location":"usage_guide/rag/reader/#top-p-sampling-nucleus-sampling","title":"Top-p Sampling (Nucleus Sampling)","text":"<ul> <li>$top_p$ allow to sample from the smallest set of tokens whose cumulative probability exceeds $p$.</li> <li>Indeed the next token is also sampled from a restricted set of token. This sets corresponds to the $top_k$ token such that $\\sum_{i=1}^k p_i=top_p$.</li> <li>The advantage of this method compared to $top_k$ is that the size of the set $k$ the next token is sampled from depends on the distribution. Therefore it leverages more information learned by the LLM.</li> </ul>"},{"location":"usage_guide/rag/reader/#mirostat","title":"Mirostat","text":"<p>Advanced sampling algorithm that maintains target perplexity (uncertainty).</p> <p>Parameters: - $\\tau$: Target perplexity - $\\eta$: Learning rate for adjustment</p> <p>Benefit: More consistent output quality across different prompts</p>"},{"location":"usage_guide/rag/reader/#repetition-penalty","title":"Repetition Penalty","text":"<p>One issue with LLM is that tend to repeat themselves. A method to reduce repetition is to penalize token that have already been used. The parameters of this method are:</p> <p>Parameters: - repeat_last_n: How many recent tokens to consider (e.g., 64) - repeat_penalty: Penalty multiplier (e.g., 1.1 = 10% penalty)</p> <p>Effect: <pre><code>Without penalty:\n\"The system is important. The system is crucial. The system is...\"\n\nWith penalty:\n\"The system is important. It plays a crucial role. This component...\"\n</code></pre></p>"},{"location":"usage_guide/rag/reader/#context-window-num_ctx","title":"Context Window (num_ctx)","text":"<p>Maximum number of tokens the model can process at once. For instance: <pre><code>num_ctx = 2048\nnum_ctx = 4096\nnum_ctx = 8192\n</code></pre></p> <p>Trade-off: - Larger: More context, better understanding, slower, more memory - Smaller: Faster, less memory, might miss important context</p>"},{"location":"usage_guide/rag/reader/#reader-strategies","title":"\ud83c\udfaf Reader strategies","text":"<p>The Reader mode as explained below influences how the chunks passed to the LLM are consumed by the LLM: - Compact: All the chunks are consumed at the same time by the LLM. - Refine: The chunks are consumed one at a time by the LLM, each time an answer is generated. The preceding answer is passed with current chunk as input to the LLM. - Tree summarize: The Chunks are grouped together in groups of same size. Each groups are then summarize. The process is then repeated with the summaries iteratively until were each a certain number of summaries. The final set of summaries is used to generated the final answer. - Simple summarize: The Chunks are grouped together then summarized. The summary is used as a context to generated the final answer.</p> <p>More information on the reader factory of Llama Index here \ud83d\udc49 Reader Factory.</p>"},{"location":"usage_guide/rag/reader/#llm-usage","title":"\ud83e\udd16 LLM Usage","text":""},{"location":"usage_guide/rag/reader/#instantiation","title":"Instantiation","text":""},{"location":"usage_guide/rag/reader/#langchain","title":"Langchain","text":"<p>To instantiate an ollama langchain model you need the only required parameter is the name of the model <code>model_name</code>:</p> <pre><code>from rago.model.configs.llm_config.langchain import LangchainOllamaConfig\nfrom rago.model.wrapper.llm_agent.llm_agent_factory import LLMAgentFactory\n\nllm_config = LangchainOllamaConfig(model_name=\"phi3:3.8b-mini-128k-instruct-q8_0\")\nllm_agent = LLMAgentFactory.make(llm_config)\nllm_agent.query(\"Who is the most famous president of the united states?\")\n</code></pre>"},{"location":"usage_guide/rag/reader/#llamaindex","title":"LlamaIndex","text":"<p>Similarly, to instantiate an ollama llama_index model you need to provide the name of the model <code>model_name</code>. The only difference is the config class to use: <code>LlamaIndexOllamaConfig</code> instead of <code>LangchainOllamaConfig</code>.</p> <pre><code>from rago.model.configs.llm_config.llama_index import LlamaIndexOllamaConfig\nfrom rago.model.wrapper.llm_agent.llm_agent_factory import LLMAgentFactory\n\nllm_config = LlamaIndexOllamaConfig(model_name=\"phi3:3.8b-mini-128k-instruct-q8_0\")\nllm_agent = LLMAgentFactory.make(llm_config)\nllm_agent.query(\"Who is the most famous president of the united states?\")\n</code></pre>"},{"location":"usage_guide/rag/reader/#usage","title":"Usage","text":"<p>Any Language model can be: - queried with a <code>string query</code>:     <pre><code>query = \"What's the weather in Toulouse?\"\nllm_agent.query(query)\n</code></pre> - chatted to with a <code>message_sequence</code>:     <pre><code>from rago.model.wrapper.llm_agent.message import Message, Role\n\nmessage_sequence = [\n    Message(\"What's the weather like in toulouse?\", Role.USER),\n    Message(\"The weather is great\", Role.BOT),\n    Message(\"Can you tell me more?\", Role.USER),\n    ]\nllm_agent.chat(message_sequence).text\n</code></pre></p>"},{"location":"usage_guide/rag/reader/#reader","title":"\ud83d\udcd7 Reader","text":""},{"location":"usage_guide/rag/reader/#simple-reader","title":"Simple Reader","text":"<p>The simple reader uses a language model to answer the query. The retrieved context and the query are added to the reader's prompt and passed to the language model to generate an answer. Below is an example usage of the simple reader using langchain:</p> <pre><code>from rago.model.configs.reader_config.langchain import LangchainReaderConfig\nfrom rago.model.configs.llm_config.langchain import LangchainOllamaConfig\nfrom rago.model.wrapper.reader.langchain_reader import LangchainWrapperReader\nfrom rago.data_objects import RetrievedContext\n\nretrieved_context = [RetrievedContext(\"The date is 2012\"), RetrievedContext(\"Thomas is going to be 12 in 2013\")]\nllm_config = LangchainOllamaConfig(model_name=\"phi3:3.8b-mini-128k-instruct-q8_0\")\nreader_config = LangchainReaderConfig(llm=llm_config)\nreader = LangchainWrapperReader.make(config = reader_config)\nprint(reader.get_reader_output(\"hello\", retrieved_context))\n</code></pre>"},{"location":"usage_guide/rag/reader/#other-reader-strategies","title":"Other Reader Strategies","text":"<p>Using llama_index allow us to choose more complex reader strategies such as <code>CompactAndRefine</code>. To do so it is possible to add to the <code>LLamaIndexReaderConfig</code>. The section Llama Index Reader Configuration Space explains that if you use the llama Index configuration Space by default it will include the reader strategies instead of using langchain.</p>"},{"location":"usage_guide/rag/reader/#reader-configuration-space","title":"\ud83d\udd2d Reader Configuration Space","text":""},{"location":"usage_guide/rag/reader/#default-reader-configuration-space","title":"Default Reader Configuration Space","text":"<p>By default we use Langchain LLM config space:</p> <p><pre><code>  model_name: CategoricalParamSpace = Field(\n      default=CategoricalParamSpace(\n          choices=[\"smollm2:1.7b\", \"qwen3:4b\", \"gemma3:4b\", \"llama3.2:3b\"],\n      ),\n  )\n  mirostat: CategoricalParamSpace = Field(\n      default=CategoricalParamSpace(\n          choices=[0, 1, 2],\n      ),\n  )\n  mirostat_eta: FloatParamSpace = Field(default=FloatParamSpace(low=0.0, high=1.0))\n  mirostat_tau: FloatParamSpace = Field(default=FloatParamSpace(low=0.0, high=1.0))\n  num_ctx: IntParamSpace = Field(default=IntParamSpace(low=64, high=12800, step=64))\n  repeat_last_n: IntParamSpace = Field(default=IntParamSpace(low=-1, high=256))\n  base_url: str = Field(default=os.environ.get(\"TEST_OLLAMA_HOST\", \"\"))\n</code></pre> See \ud83d\udc49 Default LLM Configuration Space</p>"},{"location":"usage_guide/rag/reader/#llama-index-reader-configuration-space","title":"Llama Index Reader Configuration Space","text":"<p>In order to use the Llama index reader, use this reader config space: <pre><code>from rago.optimization.search_space.reader_config_space import LlamaIndexReaderConfigSpace\nreader_config_space = LlamaIndexReaderConfigSpace()\n</code></pre></p> <p>The default configuration is the same for the LLM parameters but reader modes are added:</p> <pre><code>  type_config: CategoricalParamSpace = Field(\n      default=CategoricalParamSpace(\n          choices=[\"Refine\", \"CompactAndRefine\", \"TreeSummarize\", \"SimpleSummarize\"],\n      ),\n  )\n</code></pre>"},{"location":"usage_guide/rag/reader/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\u2699\ufe0f RAG Configurations</li> <li>\ud83e\udd16 Retriever Methods</li> </ul>"},{"location":"usage_guide/rag/retriever/","title":"Retriever methods","text":"<p>Once we have a set of chunks we can then use a retriever to select chunks useful to answer a given query.</p>"},{"location":"usage_guide/rag/retriever/#vectorindexretriever","title":"\ud83d\udd22 VectorIndexRetriever","text":"<p>The <code>Vector Index Retriever</code> is the basic retriever. The idea is to use an embedding model to compute a similarity score using cosine similarity between each chunks and the query.</p> <p>The retrieved chunks are the k chunks with the highest similarity score. Hence, the parameters are: - top k is number of retrieved chunks - similarity score threshold: the threshold between 0 and 1 in which we keep the retrieved document. - embedding: encoder language model for embedding the documents and queries</p> <p>To find relevant documents, we measure cosine similarity:</p> <p>$$\\text{similarity}(A, B) = \\frac{A \\cdot B}{|A| \\times |B|}$$</p> <p>Where: - $A \\cdot B$ = dot product, i.e. how aligned the vectors are, 1 means \"identical meaning\" therefore 0 means \"unrelated\". - $|A|$ = length of vector A - $|B|$ = length of vector B</p>"},{"location":"usage_guide/rag/retriever/#bm25-retriever","title":"\ud83d\udcca BM25 Retriever","text":"<p><code>BM25</code> stands for Best Match 25. The BM25 retriever scores each document based on the terms (words) it shares with the query using the following formula (2):</p> <p>$$BM25(D, Q) = \\sum_{i=1}^{n}IDF(q_i)\u00b7\\frac{TF(q_i, D)\u00b7(k_1+ 1)}{TF(q_i, D) +k_1\u00b7(1\u2212b+b\u00b7\\frac{|D|}{avgdl})}$$</p> <p>where: - $q_i$: The $i$-th term in the query $Q$. - $n$: The total number of terms in the query $Q$. - $TF(q_i, D)$: Term Frequency, i.e., the number of times term $q_i$ appears in document $D$. - $|D|$: The length of the document $D$ (e.g., the number of words in $D$). - $avgdl$: The average document length across all documents in the corpus. - $k1$: A tuning parameter that controls the saturation of term frequency, typically set between $1.2$ and $2.0$. - $b$: A tuning parameter that controls the degree of length normalization, typically set around $0.75$. - $IDF(q_i)$: Inverse Document Frequency, calculated as:</p> <p>$$IDF(q_i) = log\\left( \\frac{N\u2212DF(q_i) + 0.5}{DF(q_i) + 0.5} + 1 \\right)$$</p> <p>where - $N$ is the total number of documents - $DF(q_i)$ is the number of documents containing the term $q_i$.</p>"},{"location":"usage_guide/rag/retriever/#when-to-use-bm25","title":"When to Use BM25","text":"<p>Good for: - Exact term matching (product codes, model numbers) - Queries with rare, specific keywords - Multilingual scenarios (no pre-trained embeddings needed) - Explainable retrieval (can see why doc matched)</p> <p>Less good for: - Synonyms (\"car\" vs \"automobile\") - Paraphrasing - Semantic similarity</p>"},{"location":"usage_guide/rag/retriever/#vector-vs-bm25","title":"\u2705 Vector vs BM25","text":"Aspect Vector Search BM25 Type Semantic Lexical Synonyms \u2705 Understands \u274c Misses Exact Match Sometimes misses \u2705 Perfect Speed Fast (with indexing) Very fast Setup Needs embedding model No training needed Multilingual Model-dependent Works naturally <p>Best practice: Use hybrid retrieval (combine both)!</p> <p>\u26a0\ufe0f <code>Hybrid Retriever</code>, <code>Cluster Based Retriever</code> shall be implemented in next versions.</p>"},{"location":"usage_guide/rag/retriever/#retrieved-context","title":"\ud83d\udcc4 Retrieved Context","text":"<p>The retrieved context is a list of <code>RetrievedContext</code>. A retrieved context contains: - a text: chunk of information (e.g a document, a sentence or paragraph.) - with optionally its relevancy (the score) and its embedding (vector representations of the text)</p> <p>Below is an example usage of the ContextNode class:</p> <pre><code>from rago.data_objects import RetrievedContext\n\nRetrievedContext(text =\"Thomas will be twelve in 2012.\", embedding= [0, 0, 0, 0, 0, 0, 0], score = 0.8)\n</code></pre> <p>see Retrieved Context Dataclass for more details</p>"},{"location":"usage_guide/rag/retriever/#retriever-wrappers","title":"\ud83d\udd0d Retriever Wrappers","text":"<p>The retriever wrapper is a key component of the RAG pipeline it takes in the query and returns a context it deems relevant to answer the query. The retriever wrapper can be instantiated as follow:</p> <pre><code>from rago.model.wrapper.retriever.langchain_retriever import LangchainRetrieverWrapper\nfrom rago.model.configs.retriever_config.langchain import LangchainRetrieverConfig\nfrom rago.model.configs.encoder_config.langchain import HuggingFaceLangchainEncoderConfig\n\nencoder_config = HuggingFaceLangchainEncoderConfig(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\nconfig = LangchainRetrieverConfig(\n    type = \"VectorIndexRetriever\",\n    similarity_function=\"cosine\",\n    search_type=\"similarity_score_threshold\",\n    search_kwargs={\"k\":3, \"score_threshold\" : 0.0},\n    encoder = encoder_config,\n    )\nretriever = LangchainRetrieverWrapper.make(config, inputs_chunks=[\"Thomas is 12.\"])\nprint(retriever.get_retriever_output(\"Hello\"))\n</code></pre>"},{"location":"usage_guide/rag/retriever/#retrievers-configuration-space-in-rago","title":"\ud83e\udded Retrievers Configuration Space in RAGO","text":"<p>In order to have a deeper look on the retriever configuration spaces implemented in RAGO, see the following code \ud83d\udc49 \ud83d\udc0d Retriever Configuration Space python methods. All the following parameter values can be adapted as you need, and you can combine or choose one of the methods.</p>"},{"location":"usage_guide/rag/retriever/#default-retriever-configuration-space","title":"Default Retriever Configuration Space","text":"<p>By default the Retriever Configuration Space used in RAGO is based on the <code>VectorIndexRetriever</code> method with these default parameter values: <pre><code>similarity_function: CategoricalParamSpace = Field(default=CategoricalParamSpace(choices=[\"cosine\"]))\nsearch_type: CategoricalParamSpace = Field(default=CategoricalParamSpace(choices=[\"similarity_score_threshold\"]))\ntop_k: IntParamSpace = Field(default=IntParamSpace(low=1, high=5))\nscore_threshold: FloatParamSpace = Field(default=FloatParamSpace(low=0.0, high=0.9))\nencoder: HFEncoderConfigSpace = Field(default=HFEncoderConfigSpace())\n</code></pre> By default the encoder configuration space used with this retriever methods is defined by: <pre><code>    model_name: CategoricalParamSpace = Field(\n        default=CategoricalParamSpace(\n            name=\"encoder_model_name\",\n            choices=[\n                \"BAAI/bge-m3\",\n                \"intfloat/e5-large-v2\",\n                \"Qwen/Qwen3-Embedding-0.6B\",\n                \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n            ],\n        ),\n    )\n</code></pre> See the following code \ud83d\udc49 \ud83d\udc0d Encoder Configuration Space python methods for more details.</p>"},{"location":"usage_guide/rag/retriever/#bm25-configuration-space","title":"BM25 Configuration Space","text":"<p>Here is the default configuration space fo the BM25 retrievers: <pre><code>bm25_k1: FloatParamSpace = Field(default=FloatParamSpace(low=1.2, high=2.0))\nbm25_b: FloatParamSpace = Field(default=FloatParamSpace(low=0.0, high=1.0))\nbm25_similarity: CategoricalParamSpace = Field(default=CategoricalParamSpace(choices=[\"custom_bm25\"]))\n</code></pre> You can find the values $k1$ and $b$ defined in the formula (2).</p>"},{"location":"usage_guide/rag/retriever/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>\u2699\ufe0f RAG Configurations</li> <li>\ud83e\udd16 Reader Methods</li> </ul>"}]}